{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cbebca",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50181630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#smote data rebalancing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#normalization\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret import show\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ecdfa",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffc6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://archive-beta.ics.uci.edu/ml/datasets/cervical+cancer+risk+factors\n",
    "\"\"\"\n",
    "The dataset was collected at 'Hospital Universitario de Caracas' in Caracas, Venezuela.\n",
    "The dataset comprises demographic information, habits, and historic medical records of 858 patients.\n",
    "Several patients decided not to answer some of the questions because of privacy concerns (missing values).\n",
    "\"\"\"\n",
    "\n",
    "rf = pd.read_csv('risk_factors_cervical_cancer.csv',encoding='utf8')\n",
    "#rf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a76d55",
   "metadata": {},
   "source": [
    "# Covert data to usable datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc42aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with NaN\n",
    "rf = rf.replace('?',np.nan)\n",
    "\n",
    "# covert everything to float64, some classes will be converted to bool once missing values are taken care of\n",
    "for label, col in rf.iteritems():\n",
    "    rf[label] = pd.to_numeric(col, errors='coerce')\n",
    "\n",
    "#TODO: some things are still int after this and object types get transformed to bool automatically. But NaN values\n",
    "#are not falsly converted to True so that should be okay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d83df",
   "metadata": {},
   "source": [
    "# Examine data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55307b39",
   "metadata": {},
   "source": [
    "## Check missing values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f3466",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = rf.isnull().sum() * 100 / len(rf)\n",
    "missing_value_df = pd.DataFrame({'column_name': rf.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2afd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with more than 20% missing\n",
    "selection_columns = missing_value_df.loc[missing_value_df['percent_missing'] >= 20].iloc[:, 0]\n",
    "\n",
    "for col in selection_columns:\n",
    "    rf = rf.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e4535",
   "metadata": {},
   "source": [
    "## Check missing values per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count = []\n",
    "for idx in range(len(rf)):\n",
    "    missing = rf.loc[[idx]].isna().sum().sum()\n",
    "    missing_count.append(missing)\n",
    "\n",
    "removed = 0\n",
    "for idx, val in enumerate(missing_count):\n",
    "    # Remove all rows where more than 15% of the data are missing\n",
    "    # If 20% is chosen, 106 patients are excluded\n",
    "    if val >= len(rf.columns)*0.15:\n",
    "        rf = rf.drop(idx)\n",
    "        removed+=1\n",
    "print(\"{} number of rows were removed\".format(removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6b478",
   "metadata": {},
   "source": [
    "## Investigate class imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d21d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of patients per group\n",
    "rf.groupby('Biopsy').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ffe903",
   "metadata": {},
   "source": [
    "## Correlations between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-bool parameters\n",
    "#sns.pairplot(rf_imp[['Age','Number of sexual partners','Num of pregnancies','IUD (years)',\n",
    "#                 'Hormonal Contraceptives (years)', 'STDs (number)', 'Smokes (years)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe2b30",
   "metadata": {},
   "source": [
    "## Investigate STDs (number) vs STDs: Number of diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if STDs (number) and STDs: Number of diagnosis are the same\n",
    "STD_comparison = np.where((rf['STDs (number)']==rf['STDs: Number of diagnosis']), True, False)\n",
    "\n",
    "# check manual calculation with STDs(number) if STDS (number) and STDs: Number of diagnosis not the same\n",
    "if not STD_comparison.all():\n",
    "    # create a new column with the sum of all STDs\n",
    "    rf['STDs (number manual)'] = (rf['STDs:condylomatosis'] + rf['STDs:cervical condylomatosis'] +\n",
    "        rf['STDs:vaginal condylomatosis'] + rf['STDs:vulvo-perineal condylomatosis'] +\n",
    "        rf['STDs:syphilis'] + rf['STDs:pelvic inflammatory disease'] +\n",
    "        rf['STDs:genital herpes'] + rf['STDs:molluscum contagiosum'] +\n",
    "        rf['STDs:AIDS'] + rf['STDs:HIV'] +\n",
    "        rf['STDs:Hepatitis B'] + rf['STDs:HPV'])\n",
    "    \n",
    "    # if they're the same drop the new column again\n",
    "    manual_comp = np.where((rf['STDs (number)']==rf['STDs (number manual)']), True, False)\n",
    "    if manual_comp.all():\n",
    "        rf = rf.drop(['STDs (number manual)'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4005ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STDs: Number of diagnosis is also not the same as just STDs\n",
    "STD_comparison = np.where((rf['STDs']==rf['STDs: Number of diagnosis']), True, False).all() #=False\n",
    "\n",
    "# is STDs: Number of diagnosis a encoded value?\n",
    "minimum = rf['STDs: Number of diagnosis'].min() #=0\n",
    "maximum = rf['STDs: Number of diagnosis'].max() #=3\n",
    "# unlikely that it's the diagnosis code if the max is 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76084a0b",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959d8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision to not use normalization to keep it interpretable\n",
    "\"\"\"column_names  = rf.columns.values.tolist()\n",
    "\n",
    "#normalize all columns to 0 to 1\n",
    "values = rf.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "values_scaled = min_max_scaler.fit_transform(values)\n",
    "rf_norm = pd.DataFrame(values_scaled)\n",
    "\n",
    "# rename the columns again\n",
    "rf_norm.columns = column_names\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13cdd5",
   "metadata": {},
   "source": [
    "# Pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0697ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'Dx:Cancer'\n",
    "column_ = 'Dx:HPV'\n",
    "agg_function = np.mean \n",
    "\n",
    "pivot_sum=pd.pivot_table(rf,index=[group],values=[column_],aggfunc=agg_function)\n",
    "pivot_sum=pivot_sum.reset_index()\n",
    "pivot_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf9eddd",
   "metadata": {},
   "source": [
    "# Splitting the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using oversampled data\n",
    "# split dataset in features and target variable\n",
    "feature_cols = rf.columns[1:len(rf.columns)-4]\n",
    "X = rf[feature_cols] # Features\n",
    "y = rf.Biopsy # Target variable\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # 80% training and 20% test\n",
    "\n",
    "# Implement splitting for cross-validation here later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195f52e",
   "metadata": {},
   "source": [
    "## Data imputation\n",
    "Following the methods described in:\n",
    "Razali, Nazim & Mostafa, Salama & Mustapha, Aida & Abd Wahab, Mohd Helmy & Ibrahim, Nurul. (2020). Risk Factors of Cervical Cancer using Classification in Data Mining. Journal of Physics: Conference Series. 1529. 022102. 10.1088/1742-6596/1529/2/022102. \n",
    "\n",
    "\"Missing values for attribute that have integer data type were filled using the sample mean while boolean\n",
    "were filled using the sample mode.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d100b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation on X_train and X_test\n",
    "# minority class is not missing any data, known from analysis before\n",
    "\n",
    "# imputation method 1: subsitute mean/mode of column for missing values -> X_<train/test>_mean_imputation\n",
    "# imputation method 2: use unique values that don't/can't otherwise exist in the df. Chosen value: -99\n",
    "\n",
    "X_train_mean_imputation = X_train.copy()\n",
    "X_test_mean_imputation = X_test.copy()\n",
    "\n",
    "X_train_unique_imputation = X_train.copy()\n",
    "X_test_unique_imputation = X_test.copy()\n",
    "\n",
    "# store columns with specific data type\n",
    "bool_columns = ['Smokes','Hormonal Contraceptives', 'IUD', 'STDs',\n",
    "                'STDs:condylomatosis', 'STDs:cervical condylomatosis',\n",
    "                'STDs:vaginal condylomatosis', 'STDs:vulvo-perineal condylomatosis',\n",
    "                'STDs:syphilis', 'STDs:pelvic inflammatory disease', \n",
    "                'STDs:genital herpes', 'STDs:molluscum contagiosum',\n",
    "                'STDs:AIDS', 'STDs:HIV', 'STDs:Hepatitis B', 'STDs:HPV',\n",
    "                'Dx:Cancer', 'Dx:CIN', 'Dx:HPV', 'Dx']\n",
    "\n",
    "# replace NaN with mode for columns with dtype bool\n",
    "for label, col in X_train_mean_imputation.iteritems():\n",
    "    if label in bool_columns:\n",
    "        columns_mode = col.mode()\n",
    "        # method 1\n",
    "        X_train_mean_imputation[label] = col.fillna(columns_mode[0])\n",
    "        X_test_mean_imputation[label] = col.fillna(columns_mode[0])\n",
    "\n",
    "        #convert column to bool while we're at it\n",
    "        X_train_mean_imputation[label] = X_train_mean_imputation[label].astype('bool')\n",
    "        X_test_mean_imputation[label] = X_test_mean_imputation[label].astype('bool')\n",
    "    \n",
    "float_columns = X_train_mean_imputation.select_dtypes(include=['float64']).columns\n",
    "\n",
    "# replace NaN with mean for columns with dtype float\n",
    "for col in float_columns:\n",
    "    columns_mean = X_train_mean_imputation[col].mean()\n",
    "    X_train_mean_imputation[col] = X_train_mean_imputation[col].fillna(columns_mean)\n",
    "    X_test_mean_imputation[col] = X_test_mean_imputation[col].fillna(columns_mean)\n",
    "    \n",
    "# method 2:\n",
    "X_train_unique_imputation = X_train_unique_imputation.fillna(-99)\n",
    "X_test_unique_imputation = X_test_unique_imputation.fillna(-99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b5c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check minority class imbalance\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d412d20",
   "metadata": {},
   "source": [
    "# Oversampling of X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360c510",
   "metadata": {},
   "source": [
    "## Implement Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c909d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.concat([X_train_unique_imputation, y_train], axis=1) # change here which imputation method to use\n",
    "\n",
    "# locate all columns where Biopsy is 1\n",
    "minority_class = training_data.loc[training_data['Biopsy'] == 1]\n",
    "\n",
    "print(\"Minority class count before oversampling: \\n{}\\n\".format(training_data['Biopsy'].value_counts()))\n",
    "\n",
    "# oversample with factor 12.2\n",
    "minority_class = minority_class.sample(frac=12.2, replace=True, random_state=1)\n",
    "\n",
    "# concat dataframes\n",
    "frames = [training_data, minority_class]\n",
    "training_oversampled = pd.concat(frames)\n",
    "\n",
    "print(\"Minority class count after oversampling: \\n{}\".format(training_oversampled['Biopsy'].value_counts()))\n",
    "\n",
    "# split into X and y again\n",
    "X_train_oversampled = training_oversampled.iloc[:,:-1]\n",
    "y_train_oversampled = training_oversampled.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1aa555",
   "metadata": {},
   "source": [
    "## SMOTE oversampling - training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f758986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling using synthetic minority oversampling technique (SMOTE)\n",
    "# see: Chawla N V, Bowyer K W, Hall L O and Kegelmeyer W P 2002 Journal of Artificial Intelligence Research 16 321-357\n",
    "\n",
    "# for reproducibility purposes\n",
    "seed = 100\n",
    "# SMOTE number of neighbors\n",
    "k = 1\n",
    "\n",
    "sm = SMOTE(sampling_strategy='auto', k_neighbors=k, random_state=seed)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train_unique_imputation, y_train)\n",
    "\n",
    "print(\"Minority class count after SMOTE oversampling: \\n{}\".format(y_train_smote.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8e7c2",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a928e08",
   "metadata": {},
   "source": [
    "## Explainable Boosting Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an ebm\n",
    "ebm = ExplainableBoostingClassifier()\n",
    "ebm.fit(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f0cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# understand the model\n",
    "ebm_global = ebm.explain_global()\n",
    "show(ebm_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18baed84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# understand individual predictions\n",
    "ebm_local = ebm.explain_local(X_test_mean_imputation, y_test)\n",
    "show(ebm_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd2cad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Return the mean accuracy on the given test data and labels\n",
    "acc = ebm.score(X_test_mean_imputation, y_test)\n",
    "print(\"The accuracy of the model using mean oversampling is: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a ebm to the data where smote oversampling was used\n",
    "\n",
    "# using smote data\n",
    "\n",
    "# fit an ebm\n",
    "ebm_smote = ExplainableBoostingClassifier()\n",
    "ebm_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# understand the model\n",
    "#ebm_smote_global = ebm_smote.explain_global()\n",
    "#show(ebm_smote_global)\n",
    "\n",
    "# Return the mean accuracy on the given test data and labels\n",
    "acc_smote = ebm_smote.score(X_test_mean_imputation, y_test)\n",
    "print(\"The accuracy of the model is using smote oversampling is: {}\".format(acc_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170929d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
